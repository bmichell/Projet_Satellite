\documentclass[a4paper,10pt]{report}
\input{include.tex}
\DeclareUnicodeCharacter{00A0}{ }
\widowpenalty=10000
\clubpenalty=10000
\input{environnements.tex}
\setlength{\parindent}{0.5cm}
\begin{document}
\begin{multicols}{2}
\title{Etat de lard}
\author{Aurélien Barbotin Pierre David Benjamin Michelland Youna le Vaou}

\maketitle
\chapter{État de l'art}
\input{EtatDeLart}
\section{Machine Learning}
\textit{L'apprentissage automatique (machine learning en anglais), champ d'étude de l'intelligence artificielle, concerne la conception, l'analyse, le développement et l'implémentation de méthodes permettant à une machine (au sens large) d'évoluer par un processus systématique, et ainsi de remplir des tâches difficiles ou impossibles à remplir par des moyens algorithmiques plus classiques.\footnote{\href{https://fr.wikipedia.org/wiki/Apprentissage_automatique}{Apprentissage automatique}}}
\subsection{Les principes}
\paragraph{}
Il existe plusieurs familles d'algorithmes qui peuvent être utilisés pour l'apprentissage supervisé, on différenciera dans un premier temps les algorithmes linéaires et non-linéaires. Dans la famille des algorithmes non-linéaire, on trouve des algorithmes intrinsèquement multi-classes (tel que l'algorithme dit des k-plus proches voisins que nous détaillerons plus tard), et les algorithmes binaires.
\input{diagrammeML}
\paragraph{}
Les algorithmes intrinsèquement multi-classes se suffisent à eux même, dans le sens où il suffit d'appliquer l'algorithme à l'ensemble des échantillons d'entraînement pour obtenir une classification complète.
\paragraph{}
Les algorithmes binaires, quand à eux demandent un peu plus de travail, dès que l'on veut traiter plus de deux classe, il faut faire plusieurs entraînements, en se ramenant chaque fois à un problème binaire. La encore, il y deux approches possibles en fonction des algorithmes :
 \begin{itemize}
   \item[>] La première approche est appelé un contre tous, elle consiste comme son nom l'indique, à prendre chaque classe et à l'opposer à l'ensemble des autres classes. On obtient alors n problèmes binaires et pour un élément à classifier, en cas d'ambiguïté, la classe qui obtient le plus de "votes" favorables est choisie;
   \item[>] La seconde méthode est celle du un contre un, toutes les classes sont "opposées" successivement, deux à deux, et de la même façon c'est la classe qui obtient le plus de "votes" favorables qui est choisie. La principale différence réside dans le fait qu'il y a dans ce cas ${n \choose 2}=\frac{n(n-1)}{2}$ entraînements à réaliser.
 \end{itemize}
 \subsection{Les algorithmes}
\paragraph{Les classifications linéaires\\}
Réaliser une classification linéaire d'un ensemble de données en différentes classes revient, en deux dimensions, à trouver la droite qui sépare au mieux deux ensembles de vecteurs.
\input{classifLineaire}
il existe plusieurs méthodes pour trouver une droite qui sépare correctement les deux ensembles de points :
\begin{itemize}
  \item[>] La première méthode consiste à faire l'hypothèse que tous les points des deux ensembles sont alignés, et qu'on peut alors trouver une droite telle que tous les points soient à une distance de 1, pour l'une des deux classes, et de -1 pour l'autre. On se ramène alors à une optimisation linéaire qui peut-être résolue par la méthode des moindres carrés.
  \item[>] Une seconde méthode consiste à utiliser l'analyse discriminante linéaire\footnote{ou analyse discriminante de Fisher}(LDA), c'est-à-dire poser l'hypothèse que chaque ensemble de points à une distribution gaussienne, et à trouver, à partir de la variance et de la moyenne de ces distributions la meilleure séparation entre les ensembles. Cependant, cette hypothèse nous pousse à considérer uniquement un apprentissage de type un contre un et donc à faire plus d'entraînement.
\end{itemize}
\paragraph{Les classification non-linéaires}
\subparagraph{Machine à Vecteurs de Support (SVM) :}

Les SVM ont été développées dans les années 1990 d'après les travaux de Vladimir Vapnik, elle généralise les classificateurs linéaires en cherchant une séparation linéaire de marge maximum, mais en augmentant la dimension de l'espace d'étude. Par l'astuce du \textit{kernel trick}\cite{aizermanSVM}, qui consiste à remplacer le produit scalaire de l'espace considéré par l'évaluation d'une fonction, on peut ramener l'étude d'un problème non séparable linéairement, à un problème linéaire. Une classification basée sur les SVM admet donc des méta-paramètres, qui devront être fixés par l'utilisateur avant de réaliser l'entraînement. Pour optimiser au mieux ses paramètres, on utilise une méthode d'évaluation dite validation croisée qui est expliqué plus en détails par la suite\footnote{voir le paragraphe : L'évaluation}
\subparagraph{Les K plus proches voisins :}
\subsection{L'évaluation}
\paragraph{La matrice de confusion\newline}
La matrice de confusion est le principal outil de mesure de la qualité d'une classification. Pour la construire, il faut, dans un premier temps fournir un ensemble de données, dont on connaît déjà la classe, et dont on va classifier les éléments. La matrice de confusion va alors servir à synthétiser toutes les informations que l'on peut en tirer.\ref{ConfMat}
\input{MatriceConfusion}
\newpage
\paragraph{La validation croisée\newline}
Il s'agit d'une technique d'évaluation de la classification qui va nous permettre de maximiser la taille des polygones d'entraînement et de test. En effet, cette technique consiste à n'avoir qu'un seul jeu de polygones qui seront utilisés intégralement pour le test et pour l'entraînement. L'astuce réside ici à utiliser une partie des éléments du polygone pour l'entraînement et le reste pour le test, puis de réitérer en changeant les éléments utilisés pour le test et ainsi de suite jusqu'à ce que tous les éléments aient été utilisé pour le test.
\input{CrossVal}
\subsection{Les logiciels de classification}
\paragraph{QGIS}
\paragraph{}
QGIS est un logiciel libre multiplateforme, qui permet de traiter les formats usuels d'image satellite, mais aussi d'y ajouter des couches vectorielles pour délimiter des polygones ou classifier des zones géographiques.
De plus, la communauté de QGIS a développé de nombreux plugins permettant d'appliquer différents algorithmes de machine learning.\newline
Le logiciel SAGA, intégré à QGIS, utilise l'algorithme de ressemblance maximale, qui permet de faire une classification statistique des pixels, ayant choisi des polygones d’entraînement, ils vont être assimilés à des lois normales et à partir de leurs moyennes et de leurs variances, les pixel inconnus vont appartenir à la classe à laquelle ils ont le plus de chance d'appartenir.
Semi-Automatic Classification Plugin, permet également d'obtenir des classifications à partir d'image à l'aide de différents algorithmes.
OrfeoToolBox est un autre logiciel qui a la possibilité d'être utilisé via Qgis et qui peut réaliser une classification d'images satellite.

\paragraph{ENVI}\footnote{\href{http://www.exelisvis.fr/ProduitsetServices/LesproduitsENVI/ENVI.aspx}{ENVI}}
\paragraph{}
ENVI est un logiciel propriétaire payant sous licence commerciale. Il permet de traiter efficacement les données satellite à l'aide de plusieurs algorithmes dont celui de la ressemblance maximum.
\paragraph{}

\chapter{Implémentation}
\section{C++ :}
\paragraph{Les moindres carrés}
Le premier algorithme à être implémenté en C++ a été celui des moindres carrés, en effet, il est relativement simple à coder si l'on utilise une bibliothèque vectorielle comme eigen qui inclue différentes méthodes de résolution des problèmes linéaires. Nous avons ainsi pu observer notre première image de classification.\ref{firstLSE}\ref{veniseLSE}
\input{LSE}
Cette classification n'est pas extrêmement précise, mais elle permet déjà de voir que la précision atteignable avec les images sentinel-2 semble supérieur à ce qui s'est fait par le passé.
\paragraph{L'analyse discriminante linéaire}
Cette méthode est encore plus simple à mettre en \oe{}uvre que la précédente, en effet, il suffit de calculer la moyenne et la variance des polygones d'entraînement. Cependant, cette classification est aussi beaucoup moins précise que celle des moindres carrés, et la précision diminue beaucoup plus vite avec la taille des polygones.\ref{veniseLDA}
\paragraph{Les machines à vecteurs de support}
Les SVM sont plus compliquées à implémenter, c'est pour cela que nous avons recours à la bibliothèque DLIB, qui nous permet d'avoir accès à des classes déjà définit que nous devons simplement réadapter à notre problème. Mais il faut aussi mettre en place un processus de validation croisée pour choisir les méta-paramètres de cette classification.\ref{veniseSVM}
\end{multicols}
\bibliography{Biblio}
\bibliographystyle{plain}
\end{document}
